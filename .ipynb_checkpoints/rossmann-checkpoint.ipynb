{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static/img/rosslog.png\" align=\"left\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e1e663f2-7799-45cf-ba9c-1b90636b5c6e",
    "_uuid": "82ab9b775fcea7f0a7fc33486264640f69b38bf0"
   },
   "source": [
    "# Rossmann Stores Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers basics of data cleaning, EDA, feature engineering, and data modelling. For more in-deph data analysis, please consider Kaggle Kernels: https://www.kaggle.com/c/rossmann-store-sales/kernels\n",
    "\n",
    "**IMPORTANT: Make sure you do not skip any cells. In that case you risk not being able to accomplish given tasks.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rossmann Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. In this first part of the notebook we will walk you through basic steps that need to be done before start with baseline modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dd08db0b-3c0e-4a12-b77d-348b430b33d7",
    "_uuid": "82658364a5412b45f3c1bdba11c0bc17a7a9105b"
   },
   "source": [
    "### Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "acc6733e-b0f0-4d08-bd45-d50881091212",
    "_uuid": "511329117440698529039824b10aed63cf457722"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TimeGrouper' from 'pandas' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b6a705a25fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Matplotlib for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TimeGrouper' from 'pandas' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/__init__.py)"
     ]
    }
   ],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import TimeGrouper\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Display plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a97eeb3-097f-45a3-8ebf-6309e6bcd0cc",
    "_uuid": "aadc7b21d040e576b32b8d0c5e5fd2dd5e22eedd"
   },
   "outputs": [],
   "source": [
    "# Load data from CSV\n",
    "train_dataset = pd.read_csv(\"static/data/train.csv\")\n",
    "stores = pd.read_csv(\"static/data/store.csv\")\n",
    "\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data set contains sales by day for each store with the following columns:\n",
    "\n",
    "- Store - a unique id number\n",
    "- DayOfWeek/Date - the day of the week (1-7) and date (YYYY-MM-DD) for a sales data point\n",
    "- Sales - the sales for a given day\n",
    "- Customers - the number of customers on a given day. This column is highly correlated with sales and is not present in the test set.\n",
    "- Open - Values: 0 = closed, 1 = open\n",
    "- Promo - indicates whether a store was running a sales promotion that day\n",
    "- StateHoliday - Values: a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "- SchoolHoliday - indicates if a store was affected by the closure of public schools on that day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print first five rows of the \"stores\" dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stores data set contains additional columns about each store that does not vary by day:\n",
    "\n",
    "- StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "- Assortment - describes the level of products available: a = basic, b = extra, c = extended\n",
    "- CompetitionDistance - distance in meters to the nearest competitor\n",
    "- CompetitionOpenSince[Month/Year] - month/year when the nearest competitor was opened\n",
    "- Promo2 - indicator for a recurring promotion: 0 = store not participating, 1 = participating\n",
    "- Promo2Since[Week/Year] - calendar week/year when the store started participating in Promo2\n",
    "- PromoInterval - describes the intervals when Promo2 is started. E.g. \"Feb,May,Aug,Nov\" means each round starts in those months of any given year for that store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Chech out the shape of the datasets. \n",
    "# We have a lot of data observations to play around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look and understand different data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "99a7f662-57af-477a-a23c-3bfcf7ae5513",
    "_uuid": "7c499cc55c5007b8f58f5a5e1e5c64917286ff3e"
   },
   "outputs": [],
   "source": [
    "# Column datatypes\n",
    "print(train_dataset.dtypes,'\\n')\n",
    "print(stores.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "453b25d1-f99c-4b13-8310-df4f736e5c6b",
    "_uuid": "62794228d5ba90099fdc5ce2f75cc31d89708258"
   },
   "source": [
    "### Drop unwanted observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal during the data cleaning phase is to fix any problems with the provided data sets, such as:\n",
    "\n",
    "- Fix inconsistent data\n",
    "- Replace missing data with reasonable values\n",
    "- Remove Data that cannot be fixed\n",
    "- Convert categorical variables to numeric values\n",
    "- Check for outlying values, and correct them if necessary\n",
    "\n",
    "We needed to fix these types of problems so that our prediction models could be fit as accurately as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b3e01b4-8d51-4029-8f29-40a89d95508b",
    "_uuid": "5c2634eec6e5b995d822ffbd09724dad67b003d5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "train_dataset = train_dataset.drop_duplicates()\n",
    "stores = stores.drop_duplicates()\n",
    "\n",
    "# Print shape after removing duplicates\n",
    "train_dataset.shape, stores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were no duplicates. Now, let us check closed stores, stores with no customers, and stores where sales values equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "56e7d8b2-d9c0-4bba-9e38-b63fcb601814",
    "_uuid": "1d340477c6f43609eba155140b5f573ed21d6629",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop closed observation\n",
    "train_dataset = train_dataset[train_dataset.Open != 0]\n",
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a24d3b4b-4bc8-42e8-a54f-00e93645e4a2",
    "_uuid": "751727cbbce5e77bd10a12d438ee2147da5151b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print the number of stires where Customers=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b17925f-807a-405c-95ff-b04ca1bd5d5b",
    "_uuid": "3be37b0a1c44dc8ce28cff99286f2e71a1c1ac90",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset[train_dataset.Customers == 0].sort_values(by=['Store']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a423b2f-7ec7-4edd-bf83-be9a95a78cf0",
    "_uuid": "37fce11e555aa8489b936c2fd23b98ae7c9e083b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Print the number of stires where Sales=0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking the data, we decide to drop Sales == 0 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "88afa0b1-6b3e-4f95-9bf4-c05d5898a320",
    "_uuid": "64832e798c144d0336153741a6a572a60625c634",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[train_dataset.Sales != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cda420c2-f470-445f-89b6-fe8418290e96",
    "_uuid": "4761b8839c6356a1bd54d816965d7b9c79b7287b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create new feature called \"AvgPurchasing\" we will use in the second section of this notebook. Than, we summarize numerical features to understand our data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ba3adc78-df35-470e-b1e9-e1ea18a90e89",
    "_uuid": "84e322dfc683807f3614d78ea8cf1768168a7795",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset['AvgPurchasing'] = train_dataset.Sales / train_dataset.Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7395a525-596b-4c61-bb03-34085f439994",
    "_uuid": "9cb74cd7e54aa972a2d1a2d954400ed8072f2674",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Summarize numerical features\n",
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "15124d8b-ff03-4a39-a15b-986eced026e0",
    "_uuid": "cdd7c9ccf7084982e23f78e7edf4314ceb25321e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Promo2Since[Year/Week] \n",
    "# Describes the year and calendar week \n",
    "# when the store started participating in Promo2\n",
    "\n",
    "stores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb453070-a274-4c98-88ce-adae363ed6b0",
    "_uuid": "e9468ee4eb48ee9dbb90dcebe9af96fa470169aa"
   },
   "source": [
    "### Missing values of numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check for missing values and fill in any add indicator variable for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dbfd9aca-143a-4f74-9096-382bcf083331",
    "_uuid": "e32a61adf625900f6a1c5788767ddecc67610ecb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_dataset.select_dtypes(exclude=['object']).isnull().sum(),'\\n')\n",
    "print(stores.select_dtypes(exclude=['object']).isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75cbf861-551f-4526-b9c1-1ec7fb6586a9",
    "_uuid": "873fc9fab63be8290787ee57bffd74765c3364d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For the competion data, check the 3 missing CompetitionDistance\n",
    "stores[stores['CompetitionDistance'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "96ec9f59-bc6b-488e-9d6e-0c4cd47788c9",
    "_uuid": "30358b2d6c3d344b4915aa02bff2faa97b9286e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill and flag the missing numeric data\n",
    "\n",
    "stores.CompetitionOpenSinceMonth.fillna(0, inplace=True)\n",
    "stores.CompetitionOpenSinceYear.fillna(0, inplace=True)\n",
    "stores.CompetitionDistance.fillna(0, inplace=True)\n",
    "\n",
    "# Indicator variable for missing numeric data\n",
    "\n",
    "stores['CompetitionOpenSinceMonth_missing'] = stores.CompetitionOpenSinceMonth.isnull().astype(int)\n",
    "stores['CompetitionOpenSinceYear_missing'] = stores.CompetitionOpenSinceYear.isnull().astype(int)\n",
    "stores['CompetitionDistance_missing'] = stores.CompetitionDistance.isnull().astype(int)\n",
    "\n",
    "# Just fill the nan with 0 \n",
    "\n",
    "stores.Promo2SinceWeek.fillna(0, inplace=True)\n",
    "stores.Promo2SinceYear.fillna(0, inplace=True)\n",
    "stores.PromoInterval.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5006a5dc-b667-48e6-85fd-4f607bac4ead",
    "_uuid": "effe19c6152d2b5e7d65c60ea5037ddac675a8fa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5a6e11b-4a37-4752-989f-e2b206f109a3",
    "_uuid": "879ca2f254a8e66a0fbddea42b99192f314db239"
   },
   "source": [
    "### Categorical features cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have two zero values: one integer and one string. Let us fix this and have one string zero value for all StateHoliday representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2638993e-c2c9-48d0-835e-7b25c722e21a",
    "_uuid": "4fadb7eb303d9ecb34db18213f854a7c5e019e46",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display unique values of 'basement'\n",
    "train_dataset.StateHoliday.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8ba01f73-1a5b-450d-bd0e-197742ff928b",
    "_uuid": "3b37fd2256b20e07123b9bc783e270205185d41a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Your result shoud look like this: \n",
    "### array(['0', 'a', 'b', 'c'], dtype=object)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing values for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6005c505-237c-426a-895d-74618b242e8d",
    "_uuid": "d84920e69df42104eae238bc10a6f861c5336057",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display number of missing values by categorical feature\n",
    "print(train_dataset.select_dtypes(include=['object']).isnull().sum(), '\\n')\n",
    "print(stores.select_dtypes(include=['object']).isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e989bd49-2d17-451a-a1ea-693bf6452edf",
    "_uuid": "c35ce8ee3e70199f6a001a6036395c6a5c3977ac"
   },
   "source": [
    "### Joining the \"Train\" and \"Store\" tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before joining our tables, we need to set both of our table indexes. Let us set the Store values as our new table indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "81247e5f-b65d-431b-a525-a08391fd82f7",
    "_uuid": "8161c3ca119b2a614097d3486b283da405a33bfd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Set index in train_dataset to corespond to the train_dataset['Store'] values\n",
    "# Set index in stores to corespond to the stores['Store'] values\n",
    "# Drop Store column in train_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined = train_dataset.join(stores)\n",
    "df_combined = df_combined.reset_index(drop=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert store type and assortiman from char to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StateHoliday, StoreType, and Assortment, needs to be transformed into one-hot-encoding after all the cleaning and feature engineering. Let us explore each of this features unique values and replace them with int values. For example:\n",
    "\n",
    "1. StoreType ['a', 'b', 'c', 'd'] needs to be converted into [1, 2, 3, 4]\n",
    "2. Assortment ['a', 'b', 'c'] needs to be converted into [1, 2, 3]\n",
    "3. StateHoliday ['0', 'a', 'b', 'c'] needs to be converted into [0 ,1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined.StoreType.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined.Assortment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_combined.StateHoliday.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Transforme into one-hot-encoding \n",
    "# Result should look like: \n",
    "\n",
    "#### StoreType ['a', 'b', 'c', 'd'] needs to be converted into [1, 2, 3, 4]\n",
    "#### Assortment ['a', 'b', 'c'] needs to be converted into [1, 2, 3]\n",
    "#### StateHoliday ['0', 'a', 'b', 'c'] needs to be converted into [0 ,1, 2, 3]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check out your previous code\n",
    "print(\"StoreType: \", np.sort(df_combined.StoreType.unique()))\n",
    "print(\"Assortment: \", np.sort(df_combined.Assortment.unique()))\n",
    "print(\"StateHoliday: \", np.sort(df_combined.StateHoliday.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8339e1d5-cf5c-420f-bb40-3a92b3a5186e",
    "_uuid": "9c19f2bc0fca159a3cd0ba5f9683caf7ecfd5d4c"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine what type of models and predictors might work best for predicting sales, we studied which factors are causing the most variance in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variations based on Store Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest sources of variance in sales is based on the store number. In the graph below we show the average sales for the first ten stores in the data set. We can see that the average sales are quite different for each store and that there is no linear pattern. We do see stores with similar sales levels, suggesting the stores may fall into groups.\n",
    "\n",
    "The scatterplot below that shows the average sales for all the stores. We can again see that some stores have similar levels of sales, so it's likely we can group stores together when making sales predictions, which is something for which tree models are well suited.\n",
    "\n",
    "Each store number is effectively a category with its own particular sales level. When predicting future sales, we would like a single model that can make predictions for each store rather than having to fit a separate model for each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_subset = df_combined[(df_combined['Store'] < 11)]\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# Plot store sales for stores 1 to 10\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "p3 = sns.barplot(x='Store', y='Sales', data=df_subset, ax=ax)\n",
    "ax.set(xlabel='Store Number')\n",
    "ax.set_ylabel('Average Sales')\n",
    "ax.set_title('Sales for ten stores')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Create a plot of average sales per store id\n",
    "avg_sales_per_store = df_combined[['Sales', 'Store']].groupby('Store').mean()\n",
    "avg_sales_per_store.reset_index().plot(kind='scatter', x='Store', y='Sales')\n",
    "plt.xlim(0,960)\n",
    "plt.title(\"Average Sales by Store Number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set has categorical variables for different store types and the assortment of products for sale at each store. We see that these categories are a source of variance in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of Number of Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploration of the train and test sets, we ascertained that only the training data contains the number of Customers feature. The visualization below shows the positive correlation between the number of customers and sales (and also outliers). This feature is highly correlated with sales, but it’s not available until after the sales occur (i.e. it’s not in the test set). But exploring this feature helps make a case for including a proxy for the Customers feature in our final predictors (see the Feature Engineering section).\n",
    "\n",
    "We also see that there’s added information with the behavior of customers per StoreType. We see that StoreType d (shown in green) that projects to the upper left quadrant. This means that for the same level of Sales, StoreType d requires fewer customers than StoreType b (blue). There’s less clarity of StoreType impact in the middle of the cone shaped scatter, but we can certainly see the impact of Customers and Sales conditional on the StoreType mostly in the outer regions of the visualization. Thus StoreType is a relevant feature, but as a function of Customers, as we can see that around 2000 to 3000 Customers there’s a break in the cluster pattern of the middle section of the cone. Therefore, further substantiation for us to include a proxy of the Customers feature in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='Customers', y='Sales', data=df_combined, hue='StoreType',fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0190bc75-61b4-489f-bffd-71d40eed41b6",
    "_uuid": "56594318e444d4391d5cfb516eb8c955f6d11ea5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_combined, col=\"StoreType\")\n",
    "g.map(plt.scatter, \"Customers\", \"Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8974e36-d25b-457c-baaa-88de595b76d8",
    "_uuid": "5b54e070f7cde4cf7b6f4e55da6b44289506aec7"
   },
   "source": [
    "## Rossmann Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the regressive predictive nature of The Rossmann Project, we decided to approach our model methodology with a Linear Regression base model with feature selection. Note: you can also consider Ridge regression to evaluate the effects of regularization on the predictive performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The time dimension has a tremendous impact in this project. The Kaggle competition consisted in predicted the next 6 weeks (or 42 days), our splitting lead to have a testing set very large of over 900 days, explaining the fast degradation of the explained variance. These issues are the same the world of finance and economics are facing. For example the usage of an AR(n) to predict future outcomes quickly converge to a stationary states. In a real world, one would adjust the model based on the observed error. This is the spirit of tools such as the 'Kalman Filter' that are usually implemented in these domains.* \n",
    "\n",
    "*One other potential approach would consist in implementing more complicated model that naturally combine time series and decision tree, such as ART (Autoregressive Trees). The proposed method are outside of the scope of this course and thus we will focus on some other tehniques here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first baseline model, we decided to use linear regression. We used one hot encoding to convert categorical values into indicator columns. The store number is effectively a category with over 1000 different categories. We know that sales is not linear with the arbitrary numbers assigned to stores, so if this were our final model, it might have been reasonable to fit a separate regression model for each store. But since this is a baseline model, we treated the high dimension categories as if they were numeric values.\n",
    "\n",
    "In the next section we will explore how different features affect overall score. You will be challenged to spot the differences and explain what given results mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us drop some temporal features given that we are not performing time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop PromoInterval (includes Mar,Jun,Sept,Dec)\n",
    "#drop Date -> no time-series prediction\n",
    "\n",
    "no_Promo_Date = df_combined.drop(['PromoInterval', 'Date'], axis=1)\n",
    "\n",
    "# create tagret (Sales) variable \n",
    "target = no_Promo_Date['Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression: *ver. 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Sales and AvgPurchasing, but leave Customers\n",
    "drop_sales_avg_df = no_Promo_Date.drop(['Sales', 'AvgPurchasing'], axis=1)\n",
    "\n",
    "# Split the data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(drop_sales_avg_df, target, test_size=0.33)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('R2 score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.plot(y_test.values, color='red', linewidth=0.1)\n",
    "plt.plot(y_pred, color='blue', linewidth=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, explain what the above plot and its specific trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your answer: **\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression: *ver. 2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Drop Sales, but leave Customers and AvgPurchasing\n",
    "# Elaborate about given R2 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your answer: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression: *ver. 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Sales and Customers\n",
    "drop_sales_avg_df = no_Promo_Date.drop(['Sales', 'Customers'], axis=1)\n",
    "\n",
    "# Split the data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(drop_sales_avg_df, target, test_size=0.33)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('R2 score: %.2f' % r2_score(y_test, y_pred))\n",
    "\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.plot(y_test.values, color='red', linewidth=0.1)\n",
    "plt.plot(y_pred, color='blue', linewidth=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your answer: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been well established that bagging and other resampling techniques can be used to reduce the variance in model predictions. As several replicates of the original data set are created using random selection with replacement, at every step, each derivative data set is then used to construct a new model and the models are gathered together into an ensemble. To make a prediction, all of the models in the ensemble are polled and their results are averaged in the case of regression, which is our specific approach.\n",
    "\n",
    "As well, it has been well established that a powerful modeling algorithm that makes good use of bagging is Random Forests, which works by training numerous decision trees each based on a different resampling of the original training data. The random forest algorithm improves on bagging by training each tree on a random sample of the available features, to prevent each tree from choosing the same predictors.\n",
    "\n",
    "In Random Forests the bias of the full model is equivalent to the bias of a single decision tree, which itself has high variance. By creating many of these trees, a forest, and then averaging them, the variance of the final model can be greatly reduced over that of a single tree. In practice the only limitation we encountered on the size of the forest is computing time, as an infinite number of trees could be trained without ever increasing bias and with a continual - if asymptotically declining - decrease in the variance.\n",
    "\n",
    "It is for the aforementioned that we considered Random Forests as a Baseline Ensemble and its constituent, the Decision Tree Regressor as a baseline model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees were implemented as to evaluate the base consituent of the Random Forest ensemble. As per our research on previous models, Random Forest is a very good performance candidate for a baseline ensemble model for Rossmann.\n",
    "\n",
    "Decision Trees are also evaluated as a non-parametric baseline model, which enriches the comparative analysis of the Linear Regression Models.\n",
    "\n",
    "For fitting the decision tree model, we do not need to create dummy variables for categorical columns, since trees are able to make use of the factorized category values we created during data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we considered Random Forest models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Clean your data\n",
    "clean_df = df_combined.drop(['Sales', 'Customers', 'AvgPurchasing', 'PromoInterval', 'Date'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df, target, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv_optimize(clf, parameters, X, y, n_jobs=1, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, cv=n_folds, n_jobs=n_jobs, scoring=score_func)\n",
    "    else:\n",
    "        gs = GridSearchCV(clf, param_grid=parameters, n_jobs=n_jobs, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning via CV GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paramters to be tuned are:\n",
    "\n",
    "- max_depth\n",
    "- n_estimartors\n",
    "- max_features\n",
    "- random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT: This might take a while!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tuning n_estimators\n",
    "parameters = {'max_depth': [8, 9, 10], # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \n",
    "              'n_estimators': [10, 20, 30], # [ 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "              'max_features': [None], # [None, 'auto', 'sqrt', 'log2']\n",
    "              'random_state': [10, 44], # [None, 10, 44, 100]\n",
    "}\n",
    "           \n",
    "rf = RandomForestRegressor(max_depth=1, random_state=10)\n",
    "optmized_rf_trees = cv_optimize(rf, parameters, X_train, y_train, n_jobs=-1, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '\\nThe tuned parameters in the RF w/ customers, via CV grid search are:\\n'\n",
    "print 'Max tree depth: {}\\nNumber of Estimators {}\\nMax Features: {}\\nRandom State {}'.\\\n",
    "                                                                     format(optmized_rf_trees.max_depth,\n",
    "                                                                     optmized_rf_trees.n_estimators,\n",
    "                                                                     optmized_rf_trees.max_features,\n",
    "                                                                     optmized_rf_trees.random_state) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "# Run RandomForestRegressor with best params\n",
    "# Elaborate about given R2 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the best model? ** Your answer: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our final step, we will choose the best model and explore some important prediction elements. Our app accepts features and gives predictions based on users inputs. Let us see what kind of data input is neccessarry (this is important for you to setup the proper input on the Flask side). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, take one simple observation (we are using first data row):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print first row\n",
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict based on your first row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for your app\n",
    "rf.predict(np.array(X_test.iloc[0].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the above line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.predict(np.array([1, 1, 1, 0, 1, 109, 1, 1, 290.0, 10.0, 2011.0, 1, 40.0, 2014.0, 0, 0, 0]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that your input needs to be **numpy array**. Now, we can export pur model and get ready for the second part of the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf, 'rm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
